{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979c59a2",
   "metadata": {},
   "source": [
    "## Version\n",
    "\n",
    "we cannot get tensorflow==1.3.0 in python 3\\\n",
    "trying to use the compatibility module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ed97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8281137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.5.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\program files\\python39\\lib\\site-packages\n",
      "Requires: numpy, wheel, typing-extensions, astunparse, grpcio, keras-nightly, google-pasta, termcolor, tensorboard, flatbuffers, protobuf, keras-preprocessing, six, absl-py, gast, h5py, wrapt, tensorflow-estimator, opt-einsum\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049fe6f",
   "metadata": {},
   "source": [
    "## Eager execution\n",
    "TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.\n",
    "\n",
    "We are going to disable eager execution if we are to experiment with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77ad859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "# tensorflow no longer supports the simple .Session() statement\n",
    "# use tf.compat.v1.Session()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf76a5",
   "metadata": {},
   "source": [
    "## Placeholder\n",
    "accepts arguments such as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dc5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'hello world'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bce2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b298afc",
   "metadata": {},
   "source": [
    "## Math and Converting Types\n",
    "type conversion is done manually just like usual\\\n",
    "note that the values here are constants instead of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13d23cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "math_subtract = tf.subtract(tf.cast(2.0, dtype=tf.int32), 1) # casting float into int\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(math_subtract)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d12fd0",
   "metadata": {},
   "source": [
    "## Variables\n",
    "all variables will be initialized with a starting value\\\n",
    "they are used to contain weights and bias so that they will be modifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e6a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f6612",
   "metadata": {},
   "source": [
    "## Initialization with Random Values\n",
    "truncated normal will return a normally distributed set of variates in range (-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "947f679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(n_features, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(shape=n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7eb203",
   "metadata": {},
   "source": [
    "## Example Training Framework\n",
    "```python\n",
    "# Solution is available in the other \"sandbox_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "  \n",
    "    weights = tf.Variable(tf.truncated_  # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47261df4",
   "metadata": {},
   "source": [
    "## some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e0cde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    # TODO: Return weights\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=(n_features, n_labels)))\n",
    "    return weights\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    # TODO: Return biases\n",
    "    bias = tf.Variable(tf.zeros(n_labels))\n",
    "    return bias\n",
    "\n",
    "def linear(input, w, b):\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329140c",
   "metadata": {},
   "source": [
    "## mini-batching\n",
    "allows us to training a model given limited memory\\\n",
    "we divide our train dataset into equal-sized batches\\\n",
    "if the dataset size is not divisible by the mini-batch size, one of them will have a non-standard size\n",
    "```python\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "notice the first dim = None\\\n",
    "this is done to accomodate the varying size\n",
    "## batch structure\n",
    "the set of batches should follow the structure below\n",
    "```python\n",
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1024307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    pass\n",
    "    set_of_batches = []\n",
    "    \n",
    "    if len(features) % batch_size == 0:\n",
    "        num_batches = len(features) // batch_size\n",
    "    else: \n",
    "        num_batches = len(features) // batch_size + 1\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        if len(features) < num_batches:\n",
    "            ith_features = features\n",
    "            ith_labels = labels\n",
    "        else:\n",
    "            ith_features, features = features[:batch_size], features[batch_size:]\n",
    "            ith_labels, labels = labels[:batch_size], labels[batch_size:]\n",
    "        ith_batch = [ith_features, ith_labels]\n",
    "        set_of_batches.append(ith_batch)\n",
    "    return set_of_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24117bb0",
   "metadata": {},
   "source": [
    "## epochs\n",
    "an epoch is a single forward and backward pass of the whole dataset\\\n",
    "there will be one epoch every `len(dataset) / mini_batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c39d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
